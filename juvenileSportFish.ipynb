{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', 200)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Original Excel source file from the data provider stored on a GCOOS WAF folder\n",
    "sourcefile = 'https://gcoos4.geos.tamu.edu/WAF/MBON/JuvenileSportFish/2016JuvenileSportfishNOAA.xlsx'\n",
    "# Output dir\n",
    "outdir = \"D:\\\\00-GCOOS\\\\00-MBON\\\\JuvenileFish\\\\data\\\\\"\n",
    "\n",
    "# -- Read the \"Sportfish Data\" sheet from the Excel file\n",
    "# --\n",
    "df_spt = pd.read_excel(sourcefile, sheet_name='Sportfish Data')\n",
    "\n",
    "# -- Read the \"Species Code\" sheet from the Excel file\n",
    "# --\n",
    "df_spc = pd.read_excel(sourcefile, sheet_name='Species Code')\n",
    "# Remove trailing (and leading) whitespace characters\n",
    "df_spc['Species'] = df_spc['Species'].str.strip()\n",
    "\n",
    "# -- Read the \"All Data\" sheet from the Excel file\n",
    "# --\n",
    "df_all = pd.read_excel(sourcefile, sheet_name='All Data')\n",
    "# Replace special chars in column names\n",
    "df_all.columns = [col.replace('%', 'percent') for col in df_all.columns]\n",
    "\n",
    "# Drop columns that are not needed\n",
    "df_all.drop(columns=['Cyn (lengths 30-200 mm)','Cyn (lengths 30-200 mm) pres/abs only','Lug (lengths 30-260 mm)  pres/abs only'], inplace=True)\n",
    "# Rename some columns\n",
    "df_all = df_all.rename(columns={'Cyn ALL Lengths': 'cyn', 'Lug (lengths 30-260 mm)': 'lug'})\n",
    "# List of count columns to rename and melt:\n",
    "count_cols = [ 'Ari','Arp','Bab','Chf','cyn','Epi','Has','Hie','Hpa','Hyp','Lam','Lar','lug','Lum','Lun','Lus','Mym','Occ','Paa','Pab','Pal','Poc','Scb','Sco','Sev','Spa']\n",
    "# Make all lower case (to match with species code list later)\n",
    "for acol in count_cols:\n",
    "    df_all = df_all.rename(columns={acol: acol.lower()})\n",
    "counts_lower = [item.lower() for item in count_cols]\n",
    "\n",
    "# -- Melt the dataframe to long form\n",
    "# --\n",
    "# Melt species count columns to just SpeciesCode and SpeciesCount)\n",
    "df_long = pd.melt(df_all,\n",
    "                  id_vars=[col for col in df_all.columns if col not in counts_lower], # other columns to keep\n",
    "                  value_vars=counts_lower,        # species columns\n",
    "                  var_name='SpeciesCode', \n",
    "                  value_name='SpeciesCount')\n",
    "\n",
    "# -- Fix Time\n",
    "# -- \n",
    "# Fill missing times with a default\n",
    "df_long['Time'] = df_long['Time'].fillna(1200)\n",
    "# Convert to integer (from float)\n",
    "df_long['Time'] = df_long['Time'].astype('int')\n",
    "\n",
    "# -- Add a Datetime field (date and time)\n",
    "# --\n",
    "df_long['timestr'] = df_long['Time'].astype(str)\n",
    "df_long['minute'] = df_long['timestr'].str[-2:]\n",
    "# Clean dirty data: minute has values like \"78\", should probably be \"18\"\n",
    "def replace_second_last_char_if_target(s, new_char, target_char):\n",
    "    s = str(s)  # Ensure it's a string\n",
    "    if len(s) < 2 or s[-2] != target_char:\n",
    "        return s\n",
    "    return s[:-2] + new_char + s[-1]\n",
    "# DIRTY DATA: replace second last character in the minute column, '7' with '1'\n",
    "df_long['minute'] = df_long['minute'].apply(lambda x: replace_second_last_char_if_target(x, '1', '7'))\n",
    "df_long['hour'] = df_long['timestr'].str[:-2]\n",
    "df_long['hour'].replace(to_replace=\"\", value=\"00\", inplace=True)\n",
    "#df_long['Date'] = pd.to_datetime(df_long['Date'])\n",
    "df_long['Datetime'] = pd.to_datetime(df_long['Date'].dt.strftime('%Y-%m-%d') + ' ' + df_long['hour'] + ':' + df_long['Day'].astype(str) + ' ' + df_long['hour'] + ':' + df_long['minute'])\n",
    "df_long['Datetime'] = df_long['Datetime'].dt.tz_localize('EST')\n",
    "df_long['Datetime'] = df_long['Datetime'].dt.tz_convert('UTC')\n",
    "# Format datetime string ()\n",
    "df_long['Datetime'] = df_long['Datetime'].dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "# Drop the extra fields\n",
    "df_long.drop(['timestr', 'minute', 'hour', 'Date'], axis=1, inplace=True)\n",
    "\n",
    "# -- Merge Sportfish data\n",
    "# -- \n",
    "merged_df = pd.merge(df_long, df_spt, left_on=['Station','Year','Month','SpeciesCode'], right_on=['Station','Year','Month','Species Code'], how='left').copy()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "###drop_these = ['Zone_y','Species Code','Common Name', 'Code']\n",
    "###merged_df.drop(columns=drop_these, inplace=True)\n",
    "\n",
    "\n",
    "# Re-rename Zone column that was renamed by merge\n",
    "merged_df = merged_df.rename(columns={'Zone_x': 'Zone'})\n",
    "\n",
    "# -- Merge species code data \n",
    "# --\n",
    "merged_df2 = pd.merge(merged_df, df_spc, left_on=['SpeciesCode'], right_on=['Code'], how='left').copy()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "drop_these = ['Zone_y','Species Code','Common Name', 'Code']\n",
    "merged_df2.drop(columns=drop_these, inplace=True)\n",
    "\n",
    "\n",
    "# -- New dataframe: Group by Keyfield, add up total SpeciesCount \n",
    "# --\n",
    "grouped = merged_df2.groupby('Keyfield').agg({\n",
    "    'Keyfield': 'first',\n",
    "     'Station': 'first',\n",
    "     'Location': 'first',\n",
    "     'Latitude': 'first',\n",
    "     'Longitude': 'first',\n",
    "     'Year': 'first',\n",
    "     'Month': 'first',\n",
    "     'Day': 'first',\n",
    "     'Time': 'first',\n",
    "     'Zone': 'first',\n",
    "     'Area Towed': 'first',\n",
    "     'Salinity': 'first',\n",
    "     'Temperature': 'first',\n",
    "     'Grass  percent Cover rep 1': 'first',\n",
    "     'Grass  percent Cover rep 2': 'first',\n",
    "     'Grass  percent Cover rep 3': 'first',\n",
    "     'Grass  percent Cover rep 4': 'first',\n",
    "     'Grass  percent Cover rep 5': 'first',\n",
    "     'Grass  percent Cover rep 6': 'first',\n",
    "     'Grass  percent Cover rep 7': 'first',\n",
    "     'Grass  percent Cover rep 8': 'first',\n",
    "     'Grass  percent Cover rep 9': 'first',\n",
    "     'T  percent Cover rep 1': 'first',\n",
    "     'T  percent Cover rep 2': 'first',\n",
    "     'T  percent Cover rep 3': 'first',\n",
    "     'T  percent Cover rep 4': 'first',\n",
    "     'T  percent Cover rep 5': 'first',\n",
    "     'T  percent Cover rep 6': 'first',\n",
    "     'T  percent Cover rep 7': 'first',\n",
    "     'T  percent Cover rep 8': 'first',\n",
    "     'T  percent Cover rep 9': 'first',\n",
    "     'T Canopy Height rep 1': 'first',\n",
    "     'T Canopy Height rep 2': 'first',\n",
    "     'T Canopy Height rep 3': 'first',\n",
    "     'T Canopy Height rep 4': 'first',\n",
    "     'T Canopy Height rep 5': 'first',\n",
    "     'T Canopy Height rep 6': 'first',\n",
    "     'T Canopy Height rep 7': 'first',\n",
    "     'T Canopy Height rep 8': 'first',\n",
    "     'T Canopy Height rep 9': 'first',\n",
    "     'S  percent Cover rep 1': 'first',\n",
    "     'S  percent Cover rep 2': 'first',\n",
    "     'S  percent Cover rep 3': 'first',\n",
    "     'S  percent Cover rep 4': 'first',\n",
    "     'S  percent Cover rep 5': 'first',\n",
    "     'S  percent Cover rep 6': 'first',\n",
    "     'S  percent Cover rep 7': 'first',\n",
    "     'S  percent Cover rep 8': 'first',\n",
    "     'S  percent Cover rep 9': 'first',\n",
    "     'S Canopy Height rep 1': 'first',\n",
    "     'S Canopy Height rep 2': 'first',\n",
    "     'S Canopy Height rep 3': 'first',\n",
    "     'S Canopy Height rep 4': 'first',\n",
    "     'S Canopy Height rep 5': 'first',\n",
    "     'S Canopy Height rep 6': 'first',\n",
    "     'S Canopy Height rep 7': 'first',\n",
    "     'S Canopy Height rep 8': 'first',\n",
    "     'S Canopy Height rep 9': 'first',\n",
    "     'H  percent Cover rep 1': 'first',\n",
    "     'H  percent Cover rep 2': 'first',\n",
    "     'H  percent Cover rep 3': 'first',\n",
    "     'H  percent Cover rep 4': 'first',\n",
    "     'H  percent Cover rep 5': 'first',\n",
    "     'H  percent Cover rep 6': 'first',\n",
    "     'H  percent Cover rep 7': 'first',\n",
    "     'H  percent Cover rep 8': 'first',\n",
    "     'H  percent Cover rep 9': 'first',\n",
    "     'H Canopy Height Rep 1': 'first',\n",
    "     'H Canopy Height Rep 2': 'first',\n",
    "     'H Canopy Height Rep 3': 'first',\n",
    "     'H Canopy Height Rep 4': 'first',\n",
    "     'H Canopy Height Rep 5': 'first',\n",
    "     'H Canopy Height Rep 6': 'first',\n",
    "     'H Canopy Height Rep 7': 'first',\n",
    "     'H Canopy Height Rep 8': 'first',\n",
    "     'H Canopy Height Rep 9': 'first',\n",
    "     'SAV percent Cover rep 1': 'first',\n",
    "     'SAV percent Cover rep 2': 'first',\n",
    "     'SAV percent Cover rep 3': 'first',\n",
    "     'SAV percent Cover rep 4': 'first',\n",
    "     'SAV percent Cover rep 5': 'first',\n",
    "     'SAV percent Cover rep 6': 'first',\n",
    "     'SAV percent Cover rep 7': 'first',\n",
    "     'SAV percent Cover rep 8': 'first',\n",
    "     'SAV percent Cover rep 9': 'first',\n",
    "     'Total percent Cover rep 1': 'first',\n",
    "     'Total percent Cover rep 2': 'first',\n",
    "     'Total percent Cover rep 3': 'first',\n",
    "     'Total percent Cover rep 4': 'first',\n",
    "     'Total percent Cover rep 5': 'first',\n",
    "     'Total percent Cover rep 6': 'first',\n",
    "     'Total percent Cover rep 7': 'first',\n",
    "     'Total percent Cover rep 8': 'first',\n",
    "     'Total percent Cover rep 9': 'first',\n",
    "     'Algae percent Cover rep 1': 'first',\n",
    "     'Algae percent Cover rep 2': 'first',\n",
    "     'Algae percent Cover rep 3': 'first',\n",
    "     'Algae percent Cover rep 4': 'first',\n",
    "     'Algae percent Cover rep 5': 'first',\n",
    "     'Algae percent Cover rep 6': 'first',\n",
    "     'Algae percent Cover rep 7': 'first',\n",
    "     'Algae percent Cover rep 8': 'first',\n",
    "     'Algae percent Cover rep 9': 'first',\n",
    "     'Cyn Density': 'first',\n",
    "     'Average Grass percent Cover': 'first',\n",
    "     'Average T percent Cover': 'first',\n",
    "     'Average S percent Cover': 'first',\n",
    "     'Average H percent Cover': 'first',\n",
    "     'Average T Canopy Height': 'first',\n",
    "     'Average S Canopy Height': 'first',\n",
    "     'Average H Canopy Height': 'first',\n",
    "     'SpeciesCode': 'first',\n",
    "     'SpeciesCount': 'sum',\n",
    "     'Datetime': 'first',\n",
    "     'Length (mm)': 'sum',\n",
    "     'Species': 'first',\n",
    "     'Common name': 'first'\n",
    "})\n",
    "\n",
    "# Save Lines where SpeciesCount is zero: no need to have duplicates of these in the data\n",
    "zerocounts_df = grouped[grouped['SpeciesCount'] == 0].copy()\n",
    "\n",
    "# -- 1) Delete from the merged dataframe all lines with Keyfield value found in zerocounts_df \n",
    "# -- 2) Delete from the merged dataframe all lines with SpeciesCount == 0\n",
    "# -- 3) Concatenate back in one line per Keyfield that had zero total SpeciesCount (to preserve )\n",
    "# -- (No added value as all counts are 0)\n",
    "# -- \n",
    "values_to_drop = zerocounts_df['Keyfield'].unique()\n",
    "merged_df2.drop(merged_df2[merged_df2['Keyfield'].isin(values_to_drop)].index, inplace=True)\n",
    "# Delete from the merged dataframe other lines with zero counts (some species in the sample are present)\n",
    "merged_df2.drop(merged_df2[merged_df2['SpeciesCount'] == 0].index, inplace=True)\n",
    "# Concatenate with the zero count dataframe (to keep the benthic coverage data, but just one line per Keyfield / sample)\n",
    "concat_df =  pd.concat([merged_df2, zerocounts_df], ignore_index=True)\n",
    "\n",
    "# Sort by date/sample\n",
    "df_sorted = concat_df.sort_values(by='Keyfield').reset_index(drop=True)\n",
    "\n",
    "# Rename some columns\n",
    "df_sorted = df_sorted.rename(columns={'Common name': 'CommonName', 'Length (mm)': 'IndividualLength', 'Species': 'ScientificName'})\n",
    "# Get rid of spaces in column names\n",
    "for acol in df_sorted.columns.to_list():\n",
    "    df_sorted.rename({acol: re.sub(r'\\s+', '_', acol)}, axis=\"columns\", inplace=True)\n",
    "# print sorted cols, Collection Stn columns -> join 'by' which columns???\n",
    "#df_sorted.columns.to_list()\n",
    "\n",
    "#df_sorted.columns.to_list()\n",
    "cols_out = ['Datetime','Latitude','Longitude','Keyfield','Station','Location','Zone','Area_Towed',\n",
    "            'SpeciesCode','SpeciesCount','IndividualLength','ScientificName','CommonName',\n",
    "            'Salinity','Temperature','Grass_percent_Cover_rep_1','Grass_percent_Cover_rep_2',\n",
    "            'Grass_percent_Cover_rep_3', 'Grass_percent_Cover_rep_4', 'Grass_percent_Cover_rep_5',\n",
    "            'Grass_percent_Cover_rep_6','Grass_percent_Cover_rep_7','Grass_percent_Cover_rep_8',\n",
    "            'Grass_percent_Cover_rep_9','T_percent_Cover_rep_1','T_percent_Cover_rep_2',\n",
    "            'T_percent_Cover_rep_3','T_percent_Cover_rep_4', 'T_percent_Cover_rep_5', 'T_percent_Cover_rep_6',\n",
    "            'T_percent_Cover_rep_7','T_percent_Cover_rep_8','T_percent_Cover_rep_9', 'T_Canopy_Height_rep_1',\n",
    "            'T_Canopy_Height_rep_2','T_Canopy_Height_rep_3','T_Canopy_Height_rep_4','T_Canopy_Height_rep_5',\n",
    "            'T_Canopy_Height_rep_6','T_Canopy_Height_rep_7','T_Canopy_Height_rep_8','T_Canopy_Height_rep_9',\n",
    "            'S_percent_Cover_rep_1','S_percent_Cover_rep_2','S_percent_Cover_rep_3', 'S_percent_Cover_rep_4',\n",
    "            'S_percent_Cover_rep_5','S_percent_Cover_rep_6','S_percent_Cover_rep_7','S_percent_Cover_rep_8',\n",
    "            'S_percent_Cover_rep_9','S_Canopy_Height_rep_1','S_Canopy_Height_rep_2','S_Canopy_Height_rep_3',\n",
    "            'S_Canopy_Height_rep_4','S_Canopy_Height_rep_5','S_Canopy_Height_rep_6','S_Canopy_Height_rep_7',\n",
    "            'S_Canopy_Height_rep_8','S_Canopy_Height_rep_9','H_percent_Cover_rep_1','H_percent_Cover_rep_2',\n",
    "            'H_percent_Cover_rep_3','H_percent_Cover_rep_4','H_percent_Cover_rep_5','H_percent_Cover_rep_6',\n",
    "            'H_percent_Cover_rep_7','H_percent_Cover_rep_8','H_percent_Cover_rep_9','H_Canopy_Height_Rep_1',\n",
    "            'H_Canopy_Height_Rep_2','H_Canopy_Height_Rep_3','H_Canopy_Height_Rep_4','H_Canopy_Height_Rep_5',\n",
    "            'H_Canopy_Height_Rep_6','H_Canopy_Height_Rep_7','H_Canopy_Height_Rep_8','H_Canopy_Height_Rep_9',\n",
    "            'SAV_percent_Cover_rep_1','SAV_percent_Cover_rep_2','SAV_percent_Cover_rep_3',\n",
    "            'SAV_percent_Cover_rep_4','SAV_percent_Cover_rep_5','SAV_percent_Cover_rep_6','SAV_percent_Cover_rep_7',\n",
    "            'SAV_percent_Cover_rep_8','SAV_percent_Cover_rep_9','Total_percent_Cover_rep_1', 'Total_percent_Cover_rep_2',\n",
    "            'Total_percent_Cover_rep_3','Total_percent_Cover_rep_4','Total_percent_Cover_rep_5','Total_percent_Cover_rep_6',\n",
    "            'Total_percent_Cover_rep_7','Total_percent_Cover_rep_8','Total_percent_Cover_rep_9',\n",
    "            'Algae_percent_Cover_rep_1','Algae_percent_Cover_rep_2','Algae_percent_Cover_rep_3','Algae_percent_Cover_rep_4',\n",
    "            'Algae_percent_Cover_rep_5','Algae_percent_Cover_rep_6','Algae_percent_Cover_rep_7',\n",
    "            'Algae_percent_Cover_rep_8','Algae_percent_Cover_rep_9','Cyn_Density','Average_Grass_percent_Cover',\n",
    "            'Average_T_percent_Cover','Average_S_percent_Cover','Average_H_percent_Cover','Average_T_Canopy_Height',\n",
    "            'Average_S_Canopy_Height','Average_H_Canopy_Height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73f3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE DATA TO .CSV FILE:\n",
    "outdir = \"D:\\\\00-GCOOS\\\\00-MBON\\\\JuvenileFish\\\\data\\\\\"\n",
    "df_sorted[cols_out].to_csv(outdir + \"juvenilefish_sportfishlengths.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "# ERDDAP config preparation\n",
    "# --\n",
    "# Make dictionary for variable descriptions:\n",
    "# Read the header key sheet into a data frame\n",
    "df_dscrn = pd.read_excel(sourcefile, sheet_name='Header Key')\n",
    "\n",
    "# Convert the two columns to a dictionary\n",
    "# Assumption: first column is key, second column is value\n",
    "descr_dict = dict(zip(df_dscrn.iloc[:,0], df_dscrn.iloc[:,1]))\n",
    "\n",
    "# Remove keys that no longer exist in the data frame\n",
    "keys_to_remove = ['Ari', 'Arp', 'Bab', 'Chf', 'Cyn ALL Lengths', 'Cyn (lengths 30-200 mm)', 'Cyn (lengths 30-200 mm) pres/abs only', 'Epi', 'Has', 'Hie', 'Hpa', 'Hyp', 'Lam', 'Lar', 'Lug (lengths 30-260 mm)', 'Lug (lengths 30-260 mm)  pres/abs only', 'Lum', 'Lun', 'Lus', 'Mym', 'Occ', 'Paa', 'Pab', 'Pal', 'Poc', 'Scb', 'Sco', 'Sev', 'Spa']\n",
    "for key in keys_to_remove:\n",
    "    descr_dict.pop(key, None)  # Safe: does nothing if key missing\n",
    "# replace % special char with \"percent\"\n",
    "descr_dict_new = {k.replace('%', 'percent'): v.replace('%', 'percent') for k, v in descr_dict.items()}\n",
    "# replace space with underscore in the keys\n",
    "descr_dict_new = {k.replace(' ', '_'): v for k, v in descr_dict_new.items()}\n",
    "# Add missing column/variable descriptors\n",
    "descr_dict_new.update({'SpeciesCode': 'Identifier for the scientic name', 'SpeciesCount': 'Number of individuals of the species, collected in the sample', 'Datetime': 'Date and time converted to UTC from the original local time (-05)', 'IndividualLength': 'Measured length of a collected individual of the sportfish species', 'ScientificName': 'The scientific name of the collected specimen', 'CommonName': 'The common name of the collected specimen'})\n",
    "\n",
    "\n",
    "### Generate ERDDAP datasets .XML -snippet using templates according to variable type\n",
    "## Paths\n",
    "# ouput\n",
    "xml_output = outdir + \"juvenile_sportfish_xml.txt\"\n",
    "\n",
    "# template files\n",
    "xml_header_temp = \"juvenilefish_header_template.txt\"\n",
    "xml_int_temp = \"xml_template_int.txt\"\n",
    "xml_float_temp = \"xml_template_float.txt\"\n",
    "xml_string_temp = \"xml_template_string.txt\"\n",
    "\n",
    "# get types\n",
    "dt = df_sorted.dtypes.to_dict()\n",
    "int_cols = []\n",
    "float_cols = []\n",
    "string_cols = []\n",
    "\n",
    "# list types\n",
    "for akey in dt.keys():\n",
    "     if 'int' in str(dt[akey]).lower():\n",
    "        int_cols.append(akey)\n",
    "     elif 'float' in str(dt[akey]):\n",
    "         float_cols.append(akey)\n",
    "     elif 'obj' in str(dt[akey]) or 'str' in str(dt[akey]):\n",
    "         string_cols.append(akey)\n",
    "\n",
    "# These belong to the header snippet (no need to generate separately):\n",
    "header_vars = ['Datetime', 'Latitude', 'Longitude']\n",
    "\n",
    "# START concatenating the template snippets together\n",
    "# --\n",
    "\n",
    "# 1st, write the header to the output file\n",
    "with open(xml_output, \"w\") as output_file:\n",
    "    with open(xml_header_temp, \"r\") as file:\n",
    "        output_file.write(file.read())\n",
    "\n",
    "# Loop through the rest\n",
    "for acol in cols_out:\n",
    "    isPhysicalMeasurement = False\n",
    "    if acol in header_vars:\n",
    "        continue\n",
    "    elif acol in int_cols:\n",
    "        template_file = xml_int_temp\n",
    "    elif acol in float_cols:\n",
    "        template_file = xml_float_temp\n",
    "        isPhysicalMeasurement = True\n",
    "    else:\n",
    "        template_file = xml_string_temp\n",
    "    \n",
    "    # Open the xml template file for reading    \n",
    "    with open(template_file, 'r') as tempfile:\n",
    "        # Read the contents of the file\n",
    "        contents = tempfile.read()\n",
    "        # Modify the contents as needed\n",
    "        mod_contents = contents.replace('_VARNAME_', acol)\n",
    "        mod_contents = mod_contents.replace('_DESCR_', descr_dict_new[acol])\n",
    "        \n",
    "        # Try to figure out the units for some\n",
    "        if isPhysicalMeasurement:\n",
    "            if 'temperature' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'degree_Celcius')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Temperature')\n",
    "                if 'air' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'air_temperature')\n",
    "                elif 'surface' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_surface_temperature')\n",
    "                elif 'bottom' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_temperature_at_sea_floor')\n",
    "                else:\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_temperature')\n",
    "            elif 'turbidity' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'NTU')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Turbidity')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_turbidity')\n",
    "            elif 'salinity' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'PSU')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Salinity')\n",
    "                if 'surface' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_surface_salinity')\n",
    "                elif 'bottom' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_salinity_at_sea_floor')\n",
    "                else:\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_salinity')\n",
    "            #sea_water_electrical_conductivity\n",
    "            elif 'conductivity' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'S m-1')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Conductivity')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_electrical_conductivity')\n",
    "            elif 'DO' in acol or 'dissolved' in acol.lower() or 'oxygen' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'mg l-1')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Dissolved O2')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', 'mass_concentration_of_oxygen_in_sea_water')\n",
    "            elif 'height' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'mm')\n",
    "            elif 'percent' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'percent')\n",
    "            else:\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_',acol)\n",
    "\n",
    "    # Open the ouput file for appending\n",
    "    with open(xml_output, 'a') as file2:\n",
    "        # Write the modified contents back to the file\n",
    "        file2.write(mod_contents)\n",
    "\n",
    "### Add the closing tag to the output xml file:\n",
    "with open(xml_output, 'a') as output_file:\n",
    "    output_file.write(\"</dataset>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f41ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process Collection Stn Data and save it as a separate dataset\n",
    "## --\n",
    "\n",
    "# Read the \"Collection Stn Data\" sheet from the Excel file\n",
    "# --\n",
    "df_cln = pd.read_excel(sourcefile, sheet_name='Collection Stn Data')\n",
    "\n",
    "# For collection Stn data:\n",
    "# 3) Generate Keyfield column to map the data to the other published dataset: AllData + Sportfish Data\n",
    "df_cln['Keyfield'] = df_cln['Year'].astype(str).str.zfill(4) + df_cln['Month'].astype(str).str.zfill(2) + \\\n",
    "                     df_cln['Station'].astype(str).str.zfill(3)\n",
    "\n",
    "# Drop unnecessary / erroneous columns\n",
    "df_cln.drop(columns=['Year','Month','Day', 'Date'], inplace=True)\n",
    "\n",
    "# Remove units (in parentheses) from column names\n",
    "df_cln.columns = df_cln.columns.str.replace(r' \\(.+\\)', '', regex=True)\n",
    "# Remove space from column names\n",
    "df_cln.columns = df_cln.columns.str.replace(r' ', '_')\n",
    "# Remove other special char from column names\n",
    "df_cln.columns = df_cln.columns.str.replace(r'.', '', regex=False)\n",
    "\n",
    "# measurement (float) columns\n",
    "meas_cols = ['Standard_Length', 'Total_Length', 'weight', 'Length']\n",
    "\n",
    "# Convert measurement columns to float\n",
    "for acol in meas_cols:\n",
    "    df_cln[acol] = pd.to_numeric(df_cln[acol], errors='coerce')\n",
    "\n",
    "# Convert Keyfield to numeric\n",
    "df_cln['Keyfield'] = pd.to_numeric(df_cln['Keyfield'], errors='coerce')\n",
    "# 1) clean the length data: DROP rows where Length NaN\n",
    "df_cln = df_cln.dropna(subset=['Length'])\n",
    "\n",
    "# Add lat, lon, datetime field from the benthic, sportfish length dataframe\n",
    "df_right_unique = df_sorted.drop_duplicates(subset=['Keyfield'])\n",
    "df_cln = pd.merge(df_cln, df_right_unique[['Keyfield','Datetime', 'Latitude', 'Longitude']], on='Keyfield', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d47be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out Collection Stn Data to separate CSV\n",
    "cols_out = ['Datetime','Latitude','Longitude','Keyfield','Station','Genus_sp','Common_Name','Standard_Length','Total_Length','weight','Length','Species_Code']\n",
    "df_cln[cols_out].to_csv(outdir + \"juvenilefish_CollectionStnData.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "\n",
    "### Generate ERDDAP XML for Cln Stn dataset using templates according to variable type\n",
    "## Paths\n",
    "# ouput\n",
    "xml_output = outdir + \"juvenile_sportfish_collectionStn_xml.txt\"\n",
    "\n",
    "# template files\n",
    "xml_header_temp = \"juvenilefish_header_template.txt\"\n",
    "xml_int_temp = \"xml_template_int.txt\"\n",
    "xml_float_temp = \"xml_template_float.txt\"\n",
    "xml_string_temp = \"xml_template_string.txt\"\n",
    "\n",
    "# get types\n",
    "dt = df_cln.dtypes.to_dict()\n",
    "int_cols = []\n",
    "float_cols = []\n",
    "string_cols = []\n",
    "\n",
    "# list types\n",
    "for akey in dt.keys():\n",
    "     if 'int' in str(dt[akey]).lower():\n",
    "        int_cols.append(akey)\n",
    "     elif 'float' in str(dt[akey]):\n",
    "         float_cols.append(akey)\n",
    "     elif 'obj' in str(dt[akey]) or 'str' in str(dt[akey]):\n",
    "         string_cols.append(akey)\n",
    "\n",
    "# These belong to the header snippet (no need to generate separately):\n",
    "header_vars = ['Datetime', 'Latitude', 'Longitude']\n",
    "\n",
    "# START concatenating the template snippets together\n",
    "# --\n",
    "\n",
    "# 1st, write the header to the output file\n",
    "with open(xml_output, \"w\") as output_file:\n",
    "    with open(xml_header_temp, \"r\") as file:\n",
    "        output_file.write(file.read())\n",
    "\n",
    "# Loop through the rest\n",
    "for acol in cols_out:\n",
    "    isPhysicalMeasurement = False\n",
    "    if acol in header_vars:\n",
    "        continue\n",
    "    elif acol in int_cols:\n",
    "        template_file = xml_int_temp\n",
    "    elif acol in float_cols:\n",
    "        template_file = xml_float_temp\n",
    "        isPhysicalMeasurement = True\n",
    "    else:\n",
    "        template_file = xml_string_temp\n",
    "    \n",
    "    # Open the xml template file for reading\n",
    "    with open(template_file, 'r') as tempfile:\n",
    "        # Read the contents of the file\n",
    "        contents = tempfile.read()\n",
    "        # Modify the contents as needed\n",
    "        mod_contents = contents.replace('_VARNAME_', acol)\n",
    "        \n",
    "        # Try to figure out the units for some\n",
    "        if isPhysicalMeasurement:\n",
    "            if 'temperature' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'degree_Celcius')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Temperature')\n",
    "                if 'air' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'air_temperature')\n",
    "                elif 'surface' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_surface_temperature')\n",
    "                elif 'bottom' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_temperature_at_sea_floor')\n",
    "                else:\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_temperature')\n",
    "            elif 'turbidity' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'NTU')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Turbidity')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_turbidity')\n",
    "            elif 'salinity' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'PSU')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Salinity')\n",
    "                if 'surface' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_surface_salinity')\n",
    "                elif 'bottom' in acol.lower():\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_salinity_at_sea_floor')\n",
    "                else:\n",
    "                    mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_salinity')\n",
    "            #sea_water_electrical_conductivity\n",
    "            elif 'conductivity' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'S m-1')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Conductivity')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', 'sea_water_electrical_conductivity')\n",
    "            elif 'DO' in acol or 'dissolved' in acol.lower() or 'oxygen' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'mg l-1')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Dissolved O2')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', 'mass_concentration_of_oxygen_in_sea_water')\n",
    "            elif 'height' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'mm')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Other')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', acol)\n",
    "            elif 'length' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'mm')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Other')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', acol)\n",
    "            elif 'weight' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'g')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Other')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', acol)\n",
    "            elif 'percent' in acol.lower():\n",
    "                mod_contents = mod_contents.replace('_UNITS_', 'percent')\n",
    "                mod_contents = mod_contents.replace('_CATEGORY_', 'Other')\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_', acol)\n",
    "            else:\n",
    "                mod_contents = mod_contents.replace('_DESTNAME_',acol)\n",
    "\n",
    "    # Open the ouput file for appending\n",
    "    with open(xml_output, 'a') as file2:\n",
    "        # Write the modified contents back to the file\n",
    "        file2.write(mod_contents)\n",
    "\n",
    "### Add the closing tag to the output xml file:\n",
    "with open(xml_output, 'a') as output_file:\n",
    "    output_file.write(\"</dataset>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5058c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEBUG: find which variables have no description. \n",
    "# dictkeys = descr_dict_new.keys()\n",
    "# for acol in df_sorted.columns:\n",
    "#     if acol not in dictkeys:\n",
    "#         print(acol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ioos)",
   "language": "python",
   "name": "ioos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
